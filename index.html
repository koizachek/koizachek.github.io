<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DIANA KOZACHEK</title>    
    <link rel="icon" href="https://raw.githubusercontent.com/koizachek/koizachek.github.io/aiportraits/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="https://raw.githubusercontent.com/koizachek/koizachek.github.io/aiportraits/favicon.ico" type="image/x-icon">
    <style>
        @font-face {
            font-family: 'Neco-Black';
            src: url('./fonts/Neco_Complete/Fonts/WEB/fonts/Neco-Black.woff2') format('woff2'),
                 url('./fonts/Neco_Complete/Fonts/WEB/fonts/Neco-Black.woff') format('woff');            
            font-weight: normal;
            font-style: normal;
        }
        @font-face {
            font-family: 'Neco-Medium';
            src: url('./fonts/Neco_Complete/Fonts/WEB/fonts/Neco-Medium.woff2') format('woff2'),
                 url('./fonts/Neco_Complete/Fonts/WEB/fonts/Neco-Medium.woff') format('woff');            
            font-weight: normal;
            font-style: normal;
        }

        body {
            margin: 20;
            font-family: "Roboto Thin", Roboto Thin, monospace;
            font-size: 16px;
            background-size: cover; /* bildschirmfüllend */
            background-position: center; /* Hintergrund zentrieren */
            background-repeat: no-repeat; /* Kein Wiederholen */
            transition: background 0.5s ease-in-out; /* Sanfter Übergang */
            color: #431202;
            line-height: 1.6;
        }
        header, main, footer {
            padding: 50px;
            margin-bottom: 20px;
        }
        h1, h2 {
            font-family: "Playfair Display", serif; /* Schriftart für Titel */
            font-size: 24px;
            margin-bottom: 10px;
        }
        h1 {
            font-family: 'Neco-Black'; 
            font-size: 40px;
            color: #431202;
        }
        h2 {
            font-family: 'Neco-Medium'; 
            font-size: 26px;
            color: #431202;
        }
        p {
            margin-bottom: 20px;
        }
        form {
            margin-top: 20px;
        }
         /* Link-Stil */
        a {
            color: #431202; 
            text-decoration: none; /* Entfernt die Unterstreichung der Links */
            font-weight: bold;
            font-size: 16px;
        }
        /* Stil für das Bild */
        img {
            position: absolute; 
            opacity: 0.55; /* Transparenz auf 50% */
            z-index: -1; /* Hinter Text */
            width: 400px;
            height: 400px;
        }
    </style>
</head>
    </style>
</head>
<body>

    <script>
        // Bild-URLs im Repository
        var imageUrls = [
            "https://raw.githubusercontent.com/koizachek/koizachek.github.io/aiportraits/background3.JPG",
        ];

        // Zufälliges Bild
        var randomIndex = Math.floor(Math.random() * imageUrls.length);
        var randomImageUrl = imageUrls[randomIndex];
        
        // Hintergrundbild für Body
        document.body.style.backgroundImage = `url('${randomImageUrl}')`;
    </script>
    <header>
        <h1>DIANA &nbsp; KOZACHEK</h1>
<p>
  As a researcher, Diana Kozachek develops Agentic AI Systems designed to facilitate cognitive flow states and support critical thinking and problem-solving abilities. 
  As a writer, her work explores alternative configurations of thinking and working in an human-AI synergy.
  Contact: <a id="c" href="#" rel="nofollow">Kontakt</a>
</p>
<script>
  (function(){
    const u = [107,111,122,97,99,104,101,107,100,105,97,110,97].map(String.fromCharCode).join("");
    const d = [103,109,97,105,108,46,99,111,109].map(String.fromCharCode).join("");
    const el = document.getElementById("c");
    el.textContent = u + String.fromCharCode(64) + d;
    el.href = "mailto:" + el.textContent;
  })();
</script>

            <ul>
                <p><a href="https://github.com/koizachek" target="_blank">coding</a>, <a href="https://www.researchgate.net/profile/Diana-Kozachek" target="_blank">researching</a>, <a href="https://koizachek.substack.com/" target="_blank">thinking</a></p>
            </ul>
    </header>


    <main>
        <h2>SCIENTIFIC PUBLICATIONS</h2>
        <ul>
            <li><a href="https://doi.org/10.1016/j.futures.2025.103705"> Entering the Age of Hybrid Futures: A Comparative Study of Human and GPT-Generated Scenarios</a> Elsevier Futures, November 2025, DOI: 10.1016/j.futures.2025.103705</li>
            <li><a href="https://dl.acm.org/doi/10.1145/3591196.3596827">Investigating the Perception of the Future in GPT-3, -3.5 and GPT-4,</a> C&C '23: Proceedings of the 15th Conference on Creativity and Cognition, June 2023, p. 282–287. DOI: 10.1145/3591196.3596827</li>
            <li><a href="https://link.springer.com/chapter/10.1007/978-3-031-35599-8_16">Evaluating the Outcome of Collaborative VR Mind Mapping Sessions with Sentiment Analysis and Emotional Intelligence</a>, International Conference on Human-Computer Interaction HCII 2023: Human-Computer Interaction, p. 245–263. DOI: 10.1007/978-3-031-35599-8_16</li>
            <li><a href="https://ieeexplore.ieee.org/document/9582644">How Pedestrians Perceive Autonomous Buses: Evaluating Visual Signals</a>, IEEE International Conference on Human-Machine Systems, ICHMS, June 2021. DOI: 10.1109/ichms53169.2021.9582644</li>
        </ul>
        <p>

        <h2>ESSAYS</h2>
        <ul> 
            <li><a href=https://donotresearch.substack.com/p/diana-kozachek-synthoscene-chongqing>Sythoscene, Chongqing</a>, Do Not Research, 16.06.2025</li>
            <li><a href="http://texturen.net/">TOKIO - Digitale Interfaces im Stadtkörper</a>, texturen Nr. 8, UdK Verlag, 2021, p. 112-126. ISBN-10, ‎3832554653</li>
            <li><a href="http://ssdev.artsoftheworkingclass.org/edition/the-new-serenity">Arts of the Working Class</a>, Issue 17, 12 Aug 2021, Article and Artwork "Feminist Fututures"</li>
            <li><a href="https://www.hiig.de/wp-content/uploads/2019/11/IGF-Statements-Critical-Voices-WEB.pdf">Against Prediction - The Power of Imagination in the Age of Codes and Numbers</a>, Internet Governance Forum 2019: Many Worlds, Many Nets, Many Visions, Alexander von Humboldt Institute for Internet and Society, Hans Bredow Institute, 2019</li>
        </ul>
        <p>

        <h2>SPEAKING/TEACHING</h2>
        <ul>
            <li>Lecture: Developing Agentic AI Solutions for Higher Education, Universität Kassel, Germany. 18.12.2025</li>
            <li>Keynote: <a href="https://www.fit-kongress.de/agenda-2025">Future of IT Kongress (FIT)</a>, Entering the Age of Agentic AI, Augsburg, Germany 27.11.2025.</li>
            <li>Workshop: Women in IT Roundtable, Agentic AI Implementation, Switzerland 14.11.2025.</li>
            <li>Workshop: <a href="https://www.cais-research.de/creating-spaces-for-digital-futures-program/">Center for Advanced Internet Studies (CAIS)</a>, Building AI Agents for Public Good, Bochum, Germany 10.10.2025.</li>
            <li>Workshop: <a href="https://iwi.unisg.ch/en/newsuebersicht/news-detail/news/women-in-it-bruecken-zwischen-wissenschaft-und-praxis/">Women in IT Roundtable</a>, Human-Centric Agentic AI Development, Switzerland 19.05.2025.</li>
            <li>Seminar: <a href="https://iwi.unisg.ch/de/lehrstuehle/lehrstuhl-prof-dr-marco-leimeister/">Implementing AI Agents as a Service</a>, St. Gallen, Switzerland April-May 2025.</li>
            <li>Seminar: Co-Creating Multi-Modal AI Agents for Intelligent Interfaces, NTUT, Taipei, Taiwan 14.01.2025-16.01.2025.</li>
            <li><a href="https://www.linkedin.com/company/sherewires/">Panel: Immersive Technologies and Spatial Intelligence</a>, Sherewires, Shanghai, China. 16.11.2024.</li>
            <li><a href="https://www.youtube.com/watch?v=6JDa95FZvGQ&t=17s">Presentation: What doe European Futures look like according to Generative AI?</a>, WFSF 2023, World Future Studies Conference, Paris, France. 26.10.2023.</li>
            <li><a href="https://2023.hci.international/HCI-program.html">Presentation: Evaluating the Outcome of Collaborative VR Mind Mapping Sessions with Sentiment Analysis and Emotional Intelligence</a>, HCII 2023, 25th International Conference on Human-Computer Interaction, Copenhagen, Denmark 26.07.2023.</li>
            <li>Seminar: Automating the Scenario Methodology with Large Language Models. Freie University, Berlin, Germany. 21.07.2023.</li>
            <li>Keynote: <a href="https://programs.sigchi.org/c&c/2023/authors/116708">Investigating the Perception of Time in GPT-3, -3.5 and GPT-4</a>, C&C'2023, SIGCHI ACM Conference on Creativity & Cognition 2023, Virtual Event. 20.06.2023.</li>
            <li>Seminar: Evaluating Design Prototypes with Large Language Models. National Taipei University of Technology, Taipei, Taiwan 06.10.2022.</li>
            <li>Lecture: <a href="https://www.wissenschaftsjahr.de/2019/ki-camp/speakerinnen/index.html">Evaluating AI in Terms of Sustainability</a> Artificial Intelligence Camp (KI-Camp), Berlin, Germany. 05.12.2019</li>
        </ul>
        <p>
        
        <h2>PROJECTS</h2>
        <ul>
            <li>Chongqching, Synthoscene. A non-fiction book  to be published in 2027 (Work in Progress)</li>
            <li><a href="https://2038.xyz/">2038 – The New Serenity</a>. Research and Text. 2038 is a story between fact and fiction in Form of short films told from the future, showcased at the German Pavilion at the 17th International Architecture Exhibition — La Biennale di Venezia </li>
            <li><a href="https://feministfutures.net/">Feminist Futures Archive</a> is an ongoing participatory art project that initiates, creates and archives science fiction stories of a feminist internet. In workshops, talks and exhibitions, imagination and narration serve as tools for public engagement in technological processes</li>
        </ul>
        <p>
        </ul>
        <h2>READING</h2>
        <ul>
            <li><a href="https://www.cs.ucdavis.edu/~koehl/Teaching/ECS188/PDF_files/Machine_stops.pdf">The Machine Stops.</a> Edward Morgan Foster. 1909</li>
            <li><a href="https://www.sidis.net/animate.pdf">The Relation between the Tendencies. in: The Animate and the Inanimate</a>. William James Sidis. 1925</li>
            <li><a href="https://misq.umn.edu/skin/frontend/default/misq/pdf/V45I1/15887_SI_BaygiIntrona.pdf">Everything Flows: Studying Continuous Socio-Technological Transformation in a Fluid and Dynamic Digital World.</a> Reza Mousavi Baygi, Lucas Introna and Lotta Hultin. 2020.</li>
        </ul>
        <p>
        </ul>
        <p>
    </main>
    <footer>
        
        <p>&copy; 2025 Diana Kozachek.</p>
    </footer>
</body>
</html>

<script>
    function reloadPage() {
        location.reload();
    }
</script>
